{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10c4782",
   "metadata": {},
   "source": [
    "### check class distribution to decide overlap for windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d6b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =====================================================\n",
    "# LOAD AND ANALYZE DATA\n",
    "# =====================================================\n",
    "\n",
    "RAW_DATA_FILE = '/home/jupyter-yin10/EEG_HAR/Pipeline_experiments/data/combined_all.csv'\n",
    "FS = 125\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ANALYZING 10-CLASS ACTIVITY DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(RAW_DATA_FILE)\n",
    "print(f\"\\n✓ Loaded {len(df)} rows\")\n",
    "\n",
    "# =====================================================\n",
    "# 1. RECORDINGS PER ACTIVITY\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"1. RECORDINGS PER ACTIVITY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Group by activity and subject to count recordings\n",
    "recordings_per_activity = df.groupby(['activity_id', 'activity_label', 'subject']).size().reset_index(name='samples')\n",
    "\n",
    "# Count unique subjects per activity\n",
    "subjects_per_activity = recordings_per_activity.groupby(['activity_id', 'activity_label'])['subject'].nunique()\n",
    "\n",
    "print(f\"\\nNumber of subjects (recordings) per activity:\")\n",
    "for (activity_id, activity_label), n_subjects in subjects_per_activity.items():\n",
    "    print(f\"  Activity {activity_id:2d} ({activity_label:20s}): {n_subjects} subjects\")\n",
    "\n",
    "# Which activities have 6 recordings vs 5?\n",
    "activities_6_recordings = subjects_per_activity[subjects_per_activity == 6].index.get_level_values('activity_id').tolist()\n",
    "activities_5_recordings = subjects_per_activity[subjects_per_activity == 5].index.get_level_values('activity_id').tolist()\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Activities with 6 recordings (all subjects): {activities_6_recordings}\")\n",
    "print(f\"  Activities with 5 recordings (missing Subject 5): {activities_5_recordings}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. TOTAL DURATION PER ACTIVITY\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"2. TOTAL DURATION PER ACTIVITY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "duration_per_activity = df.groupby(['activity_id', 'activity_label']).size().apply(lambda x: x / FS)\n",
    "\n",
    "print(f\"\\nTotal duration per activity:\")\n",
    "for (activity_id, activity_label), duration in duration_per_activity.items():\n",
    "    is_6_rec = \"✓\" if activity_id in activities_6_recordings else \" \"\n",
    "    print(f\"  {is_6_rec} Activity {activity_id:2d} ({activity_label:20s}): {duration:6.1f}s ({duration/60:5.2f} min)\")\n",
    "\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Mean:   {duration_per_activity.mean():.1f}s\")\n",
    "print(f\"  Std:    {duration_per_activity.std():.1f}s\")\n",
    "print(f\"  Min:    {duration_per_activity.min():.1f}s\")\n",
    "print(f\"  Max:    {duration_per_activity.max():.1f}s\")\n",
    "print(f\"  Ratio (max/min): {duration_per_activity.max() / duration_per_activity.min():.2f}x\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. DURATION PER RECORDING (VARIABILITY)\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"3. DURATION PER RECORDING (VARIABILITY)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "recording_durations = df.groupby(['activity_id', 'activity_label', 'subject']).size().apply(lambda x: x / FS)\n",
    "\n",
    "print(f\"\\nMean recording duration per activity:\")\n",
    "for activity_id in sorted(df['activity_id'].unique()):\n",
    "    activity_label = df[df['activity_id'] == activity_id]['activity_label'].iloc[0]\n",
    "    activity_recs = recording_durations[activity_id]\n",
    "    \n",
    "    mean_dur = activity_recs.mean()\n",
    "    std_dur = activity_recs.std()\n",
    "    min_dur = activity_recs.min()\n",
    "    max_dur = activity_recs.max()\n",
    "    \n",
    "    print(f\"  Activity {activity_id:2d} ({activity_label:20s}): \"\n",
    "          f\"{mean_dur:5.1f}s ± {std_dur:4.1f}s  \"\n",
    "          f\"(range: {min_dur:.1f}s - {max_dur:.1f}s)\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. ESTIMATED WINDOWS WITH DIFFERENT OVERLAPS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"4. ESTIMATED WINDOWS WITH DIFFERENT OVERLAPS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "WINDOW_SIZE = 4  # seconds\n",
    "WINDOW_SAMPLES = int(WINDOW_SIZE * FS)\n",
    "\n",
    "def estimate_windows(duration_sec, overlap_pct):\n",
    "    \"\"\"Estimate number of windows from duration.\"\"\"\n",
    "    stride = WINDOW_SIZE * (1 - overlap_pct)\n",
    "    n_windows = int((duration_sec - WINDOW_SIZE) / stride) + 1\n",
    "    return max(0, n_windows)\n",
    "\n",
    "overlaps_to_test = [0.0, 0.25, 0.5, 0.75]\n",
    "\n",
    "for overlap in overlaps_to_test:\n",
    "    print(f\"\\nWith {overlap*100:.0f}% overlap:\")\n",
    "    \n",
    "    windows_per_activity = {}\n",
    "    for (activity_id, activity_label), duration in duration_per_activity.items():\n",
    "        n_windows = estimate_windows(duration, overlap)\n",
    "        windows_per_activity[activity_id] = n_windows\n",
    "        print(f\"  Activity {activity_id:2d} ({activity_label:20s}): ~{n_windows:4d} windows\")\n",
    "    \n",
    "    total_windows = sum(windows_per_activity.values())\n",
    "    min_windows = min(windows_per_activity.values())\n",
    "    max_windows = max(windows_per_activity.values())\n",
    "    \n",
    "    print(f\"  Total: {total_windows} windows\")\n",
    "    print(f\"  Imbalance ratio (max/min): {max_windows}/{min_windows} = {max_windows/min_windows:.2f}x\")\n",
    "\n",
    "# =====================================================\n",
    "# 5. VISUALIZATION\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"5. GENERATING VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Number of subjects per activity\n",
    "ax1 = axes[0, 0]\n",
    "activity_ids = sorted(df['activity_id'].unique())\n",
    "n_subjects = [subjects_per_activity.loc[aid].iloc[0] if len(subjects_per_activity.loc[aid].shape) > 0 \n",
    "              else subjects_per_activity.loc[aid] for aid in activity_ids]\n",
    "colors = ['green' if n == 6 else 'orange' for n in n_subjects]\n",
    "bars = ax1.bar(activity_ids, n_subjects, color=colors)\n",
    "ax1.set_xlabel('Activity ID', fontsize=12)\n",
    "ax1.set_ylabel('Number of Subjects (Recordings)', fontsize=12)\n",
    "ax1.set_title('Recordings per Activity', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, 7)\n",
    "ax1.set_xticks(activity_ids)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.axhline(y=5, color='orange', linestyle='--', alpha=0.5, label='5 subjects')\n",
    "ax1.axhline(y=6, color='green', linestyle='--', alpha=0.5, label='6 subjects')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Total duration per activity\n",
    "ax2 = axes[0, 1]\n",
    "durations = [duration_per_activity.loc[aid].iloc[0] if len(duration_per_activity.loc[aid].shape) > 0 \n",
    "             else duration_per_activity.loc[aid] for aid in activity_ids]\n",
    "colors = ['green' if aid in activities_6_recordings else 'orange' for aid in activity_ids]\n",
    "ax2.bar(activity_ids, durations, color=colors)\n",
    "ax2.set_xlabel('Activity ID', fontsize=12)\n",
    "ax2.set_ylabel('Total Duration (seconds)', fontsize=12)\n",
    "ax2.set_title('Total Duration per Activity', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(activity_ids)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Windows per activity with different overlaps\n",
    "ax3 = axes[1, 0]\n",
    "width = 0.2\n",
    "x = np.arange(len(activity_ids))\n",
    "for i, overlap in enumerate(overlaps_to_test):\n",
    "    windows = []\n",
    "    for aid in activity_ids:\n",
    "        dur = duration_per_activity.loc[aid].iloc[0] if len(duration_per_activity.loc[aid].shape) > 0 else duration_per_activity.loc[aid]\n",
    "        windows.append(estimate_windows(dur, overlap))\n",
    "    offset = (i - len(overlaps_to_test)/2 + 0.5) * width\n",
    "    ax3.bar(x + offset, windows, width, label=f'{overlap*100:.0f}% overlap', alpha=0.8)\n",
    "\n",
    "ax3.set_xlabel('Activity ID', fontsize=12)\n",
    "ax3.set_ylabel('Number of Windows', fontsize=12)\n",
    "ax3.set_title('Estimated Windows per Activity (Different Overlaps)', fontsize=14, fontweight='bold')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(activity_ids)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Class imbalance ratio vs overlap\n",
    "ax4 = axes[1, 1]\n",
    "overlap_range = np.linspace(0, 0.9, 50)\n",
    "imbalance_ratios = []\n",
    "for overlap in overlap_range:\n",
    "    windows = []\n",
    "    for aid in activity_ids:\n",
    "        dur = duration_per_activity.loc[aid].iloc[0] if len(duration_per_activity.loc[aid].shape) > 0 else duration_per_activity.loc[aid]\n",
    "        windows.append(estimate_windows(dur, overlap))\n",
    "    ratio = max(windows) / min(windows)\n",
    "    imbalance_ratios.append(ratio)\n",
    "\n",
    "ax4.plot(overlap_range * 100, imbalance_ratios, linewidth=2, color='steelblue')\n",
    "ax4.axvline(x=50, color='red', linestyle='--', linewidth=2, label='50% overlap (your plan)')\n",
    "ax4.set_xlabel('Overlap (%)', fontsize=12)\n",
    "ax4.set_ylabel('Imbalance Ratio (max/min)', fontsize=12)\n",
    "ax4.set_title('Class Imbalance vs Overlap', fontsize=14, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('activity_distribution_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"\\n✓ Saved visualization: activity_distribution_analysis.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# =====================================================\n",
    "# 6. RECOMMENDATIONS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"6. RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate imbalance with 50% overlap\n",
    "windows_50pct = {}\n",
    "for aid in activity_ids:\n",
    "    dur = duration_per_activity.loc[aid].iloc[0] if len(duration_per_activity.loc[aid].shape) > 0 else duration_per_activity.loc[aid]\n",
    "    windows_50pct[aid] = estimate_windows(dur, 0.5)\n",
    "\n",
    "min_windows_50 = min(windows_50pct.values())\n",
    "max_windows_50 = max(windows_50pct.values())\n",
    "imbalance_50 = max_windows_50 / min_windows_50\n",
    "\n",
    "print(f\"\\nWith 50% overlap:\")\n",
    "print(f\"  Min windows: {min_windows_50}\")\n",
    "print(f\"  Max windows: {max_windows_50}\")\n",
    "print(f\"  Imbalance ratio: {imbalance_50:.2f}x\")\n",
    "\n",
    "if imbalance_50 < 1.5:\n",
    "    print(f\"\\n✓ Imbalance is ACCEPTABLE (<1.5x)\")\n",
    "    print(f\"  Recommendation: Use 50% overlap + class weights\")\n",
    "elif imbalance_50 < 2.0:\n",
    "    print(f\"\\n⚠️  Imbalance is MODERATE (1.5-2.0x)\")\n",
    "    print(f\"  Recommendation: Use 50% overlap + class weights (should work)\")\n",
    "    print(f\"  Alternative: Consider differential overlap to balance\")\n",
    "else:\n",
    "    print(f\"\\n✗ Imbalance is SEVERE (>2.0x)\")\n",
    "    print(f\"  Recommendation: Use differential overlap to balance classes\")\n",
    "    print(f\"  Alternative: Use 50% + class weights + monitor per-class performance\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2754406",
   "metadata": {},
   "source": [
    "## Preprocessing raw data for multi class classification\n",
    "1. z score per channel\n",
    "2. change point detection to get segments\n",
    "3. segment based train test split with buffer\n",
    "4. 4s windows with 50% overlap for all class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b789b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PIPELINE: RAW → Z-SCORE → CPD → 5-FOLD → WINDOWING (10-CLASS)\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Classes: 10 (multi-class)\n",
      "  CPD: model=l2, min_size=500 samples (4s)\n",
      "  Window: 4s (500 samples)\n",
      "  CV: 5-fold temporal split\n",
      "  Test range: 18%-22%\n",
      "  Overlap: 50% for ALL classes\n",
      "\n",
      "======================================================================\n",
      "STEP 1: LOAD RAW DATA\n",
      "======================================================================\n",
      "✓ Loaded 3079330 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating recordings: 100%|██████████| 54/54 [00:00<00:00, 123.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Created 54 raw recordings\n",
      "  Subjects: ['s1', 's2', 's3', 's4', 's5', 's6']\n",
      "  Activities: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10)]\n",
      "\n",
      "Dataset: 24634.6s (410.6 min)\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Z-SCORE NORMALIZATION\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing: 100%|██████████| 54/54 [00:00<00:00, 573.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Normalized 54 recordings (z-score per channel)\n",
      "\n",
      "======================================================================\n",
      "STEP 3: CHANGE POINT DETECTION\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running CPD: 100%|██████████| 54/54 [18:20<00:00, 20.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Created 3070 segments\n",
      "\n",
      "Segment summary:\n",
      "  Mean: 8.0s ± 6.0s\n",
      "  Range: 4.0s - 57.2s\n",
      "\n",
      "======================================================================\n",
      "STEP 4: TEMPORAL 5-FOLD SPLIT\n",
      "======================================================================\n",
      "\n",
      "✓ Created 5-fold split\n",
      "✓ Total buffer segments: 112\n",
      "\n",
      "GLOBAL percentages:\n",
      "  Fold 1: Test=20.4% ✓\n",
      "  Fold 2: Test=20.8% ✓\n",
      "  Fold 3: Test=20.8% ✓\n",
      "  Fold 4: Test=20.6% ✓\n",
      "  Fold 5: Test=20.7% ✓\n",
      "\n",
      "✓ ALL FOLDS WITHIN RANGE!\n",
      "\n",
      "======================================================================\n",
      "STEP 5: CREATE WINDOWS\n",
      "======================================================================\n",
      "\n",
      "Fold 1/5... Train: 6619, Val: 1634\n",
      "    Train distribution:\n",
      "      Activity  1:  623 ( 9.4%)\n",
      "      Activity  2:  551 ( 8.3%)\n",
      "      Activity  3:  666 (10.1%)\n",
      "      Activity  4:  694 (10.5%)\n",
      "      Activity  5:  706 (10.7%)\n",
      "      Activity  6:  678 (10.2%)\n",
      "      Activity  7:  687 (10.4%)\n",
      "      Activity  8:  539 ( 8.1%)\n",
      "      Activity  9:  711 (10.7%)\n",
      "      Activity 10:  764 (11.5%)\n",
      "\n",
      "Fold 2/5... Train: 6522, Val: 1734\n",
      "    Train distribution:\n",
      "      Activity  1:  608 ( 9.3%)\n",
      "      Activity  2:  555 ( 8.5%)\n",
      "      Activity  3:  661 (10.1%)\n",
      "      Activity  4:  669 (10.3%)\n",
      "      Activity  5:  692 (10.6%)\n",
      "      Activity  6:  677 (10.4%)\n",
      "      Activity  7:  685 (10.5%)\n",
      "      Activity  8:  513 ( 7.9%)\n",
      "      Activity  9:  703 (10.8%)\n",
      "      Activity 10:  759 (11.6%)\n",
      "\n",
      "Fold 3/5... Train: 6484, Val: 1751\n",
      "    Train distribution:\n",
      "      Activity  1:  609 ( 9.4%)\n",
      "      Activity  2:  513 ( 7.9%)\n",
      "      Activity  3:  662 (10.2%)\n",
      "      Activity  4:  673 (10.4%)\n",
      "      Activity  5:  694 (10.7%)\n",
      "      Activity  6:  675 (10.4%)\n",
      "      Activity  7:  685 (10.6%)\n",
      "      Activity  8:  530 ( 8.2%)\n",
      "      Activity  9:  697 (10.7%)\n",
      "      Activity 10:  746 (11.5%)\n",
      "\n",
      "Fold 4/5... Train: 6476, Val: 1739\n",
      "    Train distribution:\n",
      "      Activity  1:  593 ( 9.2%)\n",
      "      Activity  2:  535 ( 8.3%)\n",
      "      Activity  3:  666 (10.3%)\n",
      "      Activity  4:  670 (10.3%)\n",
      "      Activity  5:  700 (10.8%)\n",
      "      Activity  6:  652 (10.1%)\n",
      "      Activity  7:  684 (10.6%)\n",
      "      Activity  8:  519 ( 8.0%)\n",
      "      Activity  9:  706 (10.9%)\n",
      "      Activity 10:  751 (11.6%)\n",
      "\n",
      "Fold 5/5... Train: 6426, Val: 1745\n",
      "    Train distribution:\n",
      "      Activity  1:  600 ( 9.3%)\n",
      "      Activity  2:  519 ( 8.1%)\n",
      "      Activity  3:  667 (10.4%)\n",
      "      Activity  4:  655 (10.2%)\n",
      "      Activity  5:  677 (10.5%)\n",
      "      Activity  6:  668 (10.4%)\n",
      "      Activity  7:  671 (10.4%)\n",
      "      Activity  8:  521 ( 8.1%)\n",
      "      Activity  9:  700 (10.9%)\n",
      "      Activity 10:  748 (11.6%)\n",
      "\n",
      "✓ Saved: /home/jupyter-yin10/EEG_HAR/Pipeline_experiments/data/windowed_zscore_temporal5fold_50overlap_10class/cv_splits.pkl\n",
      "\n",
      "======================================================================\n",
      "COMPLETED\n",
      "======================================================================\n",
      "\n",
      "Output: /home/jupyter-yin10/EEG_HAR/Pipeline_experiments/data/windowed_zscore_temporal5fold_50overlap_10class\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import ruptures as rpt\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# =====================================================\n",
    "# CONFIGURATION\n",
    "# =====================================================\n",
    "\n",
    "RAW_DATA_FILE = '/home/jupyter-yin10/EEG_HAR/Pipeline_experiments/data/combined_all.csv'\n",
    "OUTPUT_DIR = '/home/jupyter-yin10/EEG_HAR/Pipeline_experiments/data/windowed_zscore_temporal5fold_50overlap_10class'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# CPD parameters (same as before)\n",
    "FS = 125\n",
    "CPD_MODEL = 'l2'\n",
    "CPD_MIN_SIZE = int(4 * FS)\n",
    "CPD_JUMP = 5\n",
    "CPD_PEN = 10\n",
    "\n",
    "# Windowing parameters\n",
    "WINDOW_SIZE = 4\n",
    "WINDOW_SAMPLES = int(WINDOW_SIZE * FS)\n",
    "\n",
    "# Cross-validation\n",
    "N_FOLDS = 5\n",
    "ACCEPTABLE_RANGE = (0.18, 0.22)\n",
    "\n",
    "# Overlap configuration - 50% for ALL classes\n",
    "OVERLAP = 0.5\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PIPELINE: RAW → Z-SCORE → CPD → 5-FOLD → WINDOWING (10-CLASS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Classes: 10 (multi-class)\")\n",
    "print(f\"  CPD: model={CPD_MODEL}, min_size={CPD_MIN_SIZE} samples (4s)\")\n",
    "print(f\"  Window: {WINDOW_SIZE}s ({WINDOW_SAMPLES} samples)\")\n",
    "print(f\"  CV: {N_FOLDS}-fold temporal split\")\n",
    "print(f\"  Test range: {ACCEPTABLE_RANGE[0]*100:.0f}%-{ACCEPTABLE_RANGE[1]*100:.0f}%\")\n",
    "print(f\"  Overlap: {OVERLAP*100:.0f}% for ALL classes\")\n",
    "\n",
    "# =====================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =====================================================\n",
    "\n",
    "def detect_change_points(data, min_size, pen, jump):\n",
    "    \"\"\"Detect change points using Pelt algorithm with L2 cost.\"\"\"\n",
    "    signal_variance = np.var(data, axis=1)\n",
    "    algo = rpt.Pelt(model='l2', min_size=min_size, jump=jump)\n",
    "    algo.fit(signal_variance.reshape(-1, 1))\n",
    "    breakpoints = algo.predict(pen=pen)\n",
    "    return breakpoints\n",
    "\n",
    "\n",
    "def create_windows_from_segment(segment, window_size, overlap):\n",
    "    \"\"\"Create windows from ONE segment with specified overlap.\"\"\"\n",
    "    n_samples, n_channels = segment.shape\n",
    "    stride = int(window_size * (1 - overlap))\n",
    "    \n",
    "    windows = []\n",
    "    start = 0\n",
    "    \n",
    "    while start + window_size <= n_samples:\n",
    "        window = segment[start:start + window_size, :]\n",
    "        windows.append(window)\n",
    "        start += stride\n",
    "    \n",
    "    return np.array(windows) if len(windows) > 0 else np.array([]).reshape(0, window_size, n_channels)\n",
    "\n",
    "\n",
    "def select_test_segments_full_enforcement(segments, fold, n_folds, \n",
    "                                         acceptable_range=(0.18, 0.22)):\n",
    "    \"\"\"FULL ENFORCEMENT: Guarantees test percentage within acceptable_range.\"\"\"\n",
    "    total_duration = sum(seg['duration_sec'] for seg in segments)\n",
    "    \n",
    "    start_pct = (fold - 1) / n_folds\n",
    "    end_pct = fold / n_folds\n",
    "    target_start_time = total_duration * start_pct\n",
    "    target_end_time = total_duration * end_pct\n",
    "    \n",
    "    test_indices = []\n",
    "    test_duration = 0\n",
    "    buffer_indices = []\n",
    "    \n",
    "    cumulative_time = 0\n",
    "    for idx, seg in enumerate(segments):\n",
    "        seg_start = cumulative_time\n",
    "        seg_end = cumulative_time + seg['duration_sec']\n",
    "        \n",
    "        if seg_end > target_start_time and seg_start < target_end_time:\n",
    "            new_test_duration = test_duration + seg['duration_sec']\n",
    "            new_test_pct = new_test_duration / total_duration\n",
    "            \n",
    "            if new_test_pct <= acceptable_range[1]:\n",
    "                test_indices.append(idx)\n",
    "                test_duration = new_test_duration\n",
    "            else:\n",
    "                buffer_indices.append(idx)\n",
    "                break\n",
    "        \n",
    "        cumulative_time = seg_end\n",
    "    \n",
    "    final_test_pct = test_duration / total_duration if total_duration > 0 else 0\n",
    "    \n",
    "    if final_test_pct < acceptable_range[0]:\n",
    "        if len(buffer_indices) > 0:\n",
    "            buffer_seg = buffer_indices[0]\n",
    "            new_test_duration = test_duration + segments[buffer_seg]['duration_sec']\n",
    "            new_test_pct = new_test_duration / total_duration\n",
    "            \n",
    "            dist_without = acceptable_range[0] - final_test_pct\n",
    "            dist_with = new_test_pct - acceptable_range[1] if new_test_pct > acceptable_range[1] else 0\n",
    "            \n",
    "            if dist_with <= dist_without:\n",
    "                test_indices.append(buffer_seg)\n",
    "                test_duration = new_test_duration\n",
    "                buffer_indices = []\n",
    "                final_test_pct = new_test_pct\n",
    "    \n",
    "    train_indices = [i for i in range(len(segments)) \n",
    "                    if i not in test_indices and i not in buffer_indices]\n",
    "    \n",
    "    if fold == n_folds and len(train_indices) > 0 and len(test_indices) > 0:\n",
    "        max_train = max(train_indices)\n",
    "        min_test = min(test_indices)\n",
    "        \n",
    "        if max_train + 1 == min_test and len(buffer_indices) == 0:\n",
    "            test_pct_current = test_duration / total_duration\n",
    "            \n",
    "            if test_pct_current >= acceptable_range[0]:\n",
    "                train_indices.remove(max_train)\n",
    "                buffer_indices.append(max_train)\n",
    "    \n",
    "    test_duration = sum(segments[i]['duration_sec'] for i in test_indices)\n",
    "    train_duration = sum(segments[i]['duration_sec'] for i in train_indices)\n",
    "    buffer_duration = sum(segments[i]['duration_sec'] for i in buffer_indices)\n",
    "    \n",
    "    actual_test_pct = test_duration / total_duration if total_duration > 0 else 0\n",
    "    actual_train_pct = train_duration / total_duration if total_duration > 0 else 0\n",
    "    actual_buffer_pct = buffer_duration / total_duration if total_duration > 0 else 0\n",
    "    \n",
    "    buffer_applied = len(buffer_indices) > 0\n",
    "    \n",
    "    return test_indices, train_indices, buffer_indices, actual_test_pct, actual_train_pct, actual_buffer_pct, buffer_applied\n",
    "\n",
    "\n",
    "def create_windows_for_fold(fold, segments_by_recording, fold_info_df, overlap):\n",
    "    \"\"\"Create windows with uniform overlap for all classes.\"\"\"\n",
    "    train_windows = []\n",
    "    train_labels = []\n",
    "    val_windows = []\n",
    "    val_labels = []\n",
    "    \n",
    "    fold_recordings = fold_info_df[fold_info_df['fold'] == fold]\n",
    "    \n",
    "    for _, rec_fold_info in fold_recordings.iterrows():\n",
    "        rec_id = rec_fold_info['recording_id']\n",
    "        recording_segments = segments_by_recording[rec_id]\n",
    "        \n",
    "        # Train segments\n",
    "        for seg_idx in rec_fold_info['train_segment_indices']:\n",
    "            segment = recording_segments[seg_idx]\n",
    "            label = segment['activity_id']\n",
    "            \n",
    "            windows = create_windows_from_segment(segment['segment_data'], WINDOW_SAMPLES, overlap)\n",
    "            \n",
    "            if len(windows) > 0:\n",
    "                train_windows.extend(windows)\n",
    "                train_labels.extend([label] * len(windows))\n",
    "        \n",
    "        # Test segments\n",
    "        for seg_idx in rec_fold_info['test_segment_indices']:\n",
    "            segment = recording_segments[seg_idx]\n",
    "            label = segment['activity_id']\n",
    "            \n",
    "            windows = create_windows_from_segment(segment['segment_data'], WINDOW_SAMPLES, overlap)\n",
    "            \n",
    "            if len(windows) > 0:\n",
    "                val_windows.extend(windows)\n",
    "                val_labels.extend([label] * len(windows))\n",
    "    \n",
    "    return np.array(train_windows), np.array(train_labels), np.array(val_windows), np.array(val_labels)\n",
    "\n",
    "# =====================================================\n",
    "# STEP 1: LOAD RAW DATA\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: LOAD RAW DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df = pd.read_csv(RAW_DATA_FILE)\n",
    "print(f\"✓ Loaded {len(df)} rows\")\n",
    "\n",
    "eeg_columns = [f'ch{i}' for i in range(1, 17)]\n",
    "grouped = df.groupby(['subject', 'activity_id', 'activity_label'])\n",
    "\n",
    "raw_recordings = []\n",
    "\n",
    "for (subject, activity_id, activity_label), group in tqdm(grouped, desc=\"Creating recordings\"):\n",
    "    eeg_data = group[eeg_columns].values\n",
    "    duration_sec = len(eeg_data) / FS\n",
    "    \n",
    "    raw_recordings.append({\n",
    "        'subject': subject,\n",
    "        'activity_id': activity_id,\n",
    "        'activity_label': activity_label,\n",
    "        'data': eeg_data,\n",
    "        'duration_sec': duration_sec\n",
    "    })\n",
    "\n",
    "print(f\"\\n✓ Created {len(raw_recordings)} raw recordings\")\n",
    "\n",
    "subjects_in_data = sorted(list(set([item['subject'] for item in raw_recordings])))\n",
    "activities_in_data = sorted(list(set([item['activity_id'] for item in raw_recordings])))\n",
    "print(f\"  Subjects: {subjects_in_data}\")\n",
    "print(f\"  Activities: {activities_in_data}\")\n",
    "\n",
    "total_dataset_duration = sum(item['duration_sec'] for item in raw_recordings)\n",
    "print(f\"\\nDataset: {total_dataset_duration:.1f}s ({total_dataset_duration/60:.1f} min)\")\n",
    "\n",
    "# =====================================================\n",
    "# STEP 2: Z-SCORE NORMALIZATION\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: Z-SCORE NORMALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for recording in tqdm(raw_recordings, desc=\"Normalizing\"):\n",
    "    data = recording['data']\n",
    "    \n",
    "    mean_per_channel = np.mean(data, axis=0, keepdims=True)\n",
    "    std_per_channel = np.std(data, axis=0, keepdims=True) + 1e-8\n",
    "    \n",
    "    normalized_data = (data - mean_per_channel) / std_per_channel\n",
    "    recording['data'] = normalized_data\n",
    "\n",
    "print(f\"✓ Normalized {len(raw_recordings)} recordings (z-score per channel)\")\n",
    "\n",
    "# =====================================================\n",
    "# STEP 3: CHANGE POINT DETECTION\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: CHANGE POINT DETECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_segments = []\n",
    "segment_counter = 0\n",
    "\n",
    "for rec_idx, recording in enumerate(tqdm(raw_recordings, desc=\"Running CPD\")):\n",
    "    subject = recording['subject']\n",
    "    activity_id = recording['activity_id']\n",
    "    activity_label = recording['activity_label']\n",
    "    data = recording['data']\n",
    "    \n",
    "    try:\n",
    "        breakpoints = detect_change_points(data, CPD_MIN_SIZE, CPD_PEN, CPD_JUMP)\n",
    "        \n",
    "        start = 0\n",
    "        for end in breakpoints:\n",
    "            segment_data = data[start:end, :]\n",
    "            segment_duration = (end - start) / FS\n",
    "            \n",
    "            all_segments.append({\n",
    "                'segment_id': segment_counter,\n",
    "                'recording_id': rec_idx,\n",
    "                'subject': subject,\n",
    "                'activity_id': activity_id,\n",
    "                'activity_label': activity_label,\n",
    "                'segment_data': segment_data,\n",
    "                'n_samples': end - start,\n",
    "                'duration_sec': segment_duration,\n",
    "                'start_time_in_recording': start / FS\n",
    "            })\n",
    "            segment_counter += 1\n",
    "            start = end\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ CPD failed for {subject}, Activity {activity_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✓ Created {len(all_segments)} segments\")\n",
    "\n",
    "segments_df = pd.DataFrame([\n",
    "    {k: v for k, v in seg.items() if k != 'segment_data'}\n",
    "    for seg in all_segments\n",
    "])\n",
    "\n",
    "print(f\"\\nSegment summary:\")\n",
    "print(f\"  Mean: {segments_df['duration_sec'].mean():.1f}s ± {segments_df['duration_sec'].std():.1f}s\")\n",
    "print(f\"  Range: {segments_df['duration_sec'].min():.1f}s - {segments_df['duration_sec'].max():.1f}s\")\n",
    "\n",
    "segments_metadata_file = os.path.join(OUTPUT_DIR, 'segments_metadata.csv')\n",
    "segments_df.to_csv(segments_metadata_file, index=False)\n",
    "\n",
    "# =====================================================\n",
    "# STEP 4: TEMPORAL 5-FOLD SPLIT\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: TEMPORAL 5-FOLD SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "segments_by_recording = {}\n",
    "for seg in all_segments:\n",
    "    rec_id = seg['recording_id']\n",
    "    if rec_id not in segments_by_recording:\n",
    "        segments_by_recording[rec_id] = []\n",
    "    segments_by_recording[rec_id].append(seg)\n",
    "\n",
    "fold_info_per_recording = []\n",
    "total_buffer_segments = 0\n",
    "\n",
    "for rec_id in range(len(raw_recordings)):\n",
    "    recording_segments = segments_by_recording.get(rec_id, [])\n",
    "    \n",
    "    if len(recording_segments) == 0:\n",
    "        continue\n",
    "    \n",
    "    recording = raw_recordings[rec_id]\n",
    "    \n",
    "    for fold in range(1, N_FOLDS + 1):\n",
    "        test_indices, train_indices, buffer_indices, test_pct, train_pct, buffer_pct, buffer_applied = \\\n",
    "            select_test_segments_full_enforcement(\n",
    "                recording_segments,\n",
    "                fold,\n",
    "                N_FOLDS,\n",
    "                acceptable_range=ACCEPTABLE_RANGE\n",
    "            )\n",
    "        \n",
    "        total_buffer_segments += len(buffer_indices)\n",
    "        \n",
    "        fold_info_per_recording.append({\n",
    "            'recording_id': rec_id,\n",
    "            'subject': recording['subject'],\n",
    "            'activity_id': recording['activity_id'],\n",
    "            'activity_label': recording['activity_label'],\n",
    "            'fold': fold,\n",
    "            'test_segment_indices': test_indices,\n",
    "            'train_segment_indices': train_indices,\n",
    "            'buffer_segment_indices': buffer_indices,\n",
    "            'buffer_applied': buffer_applied,\n",
    "            'n_test_segments': len(test_indices),\n",
    "            'n_train_segments': len(train_indices),\n",
    "            'n_buffer_segments': len(buffer_indices),\n",
    "            'test_duration': sum(recording_segments[i]['duration_sec'] for i in test_indices),\n",
    "            'train_duration': sum(recording_segments[i]['duration_sec'] for i in train_indices),\n",
    "            'buffer_duration': sum(recording_segments[i]['duration_sec'] for i in buffer_indices),\n",
    "            'recording_duration': recording['duration_sec'],\n",
    "            'test_pct': test_pct * 100,\n",
    "            'train_pct': train_pct * 100,\n",
    "            'buffer_pct': buffer_pct * 100\n",
    "        })\n",
    "\n",
    "fold_info_df = pd.DataFrame(fold_info_per_recording)\n",
    "\n",
    "print(f\"\\n✓ Created {N_FOLDS}-fold split\")\n",
    "print(f\"✓ Total buffer segments: {total_buffer_segments}\")\n",
    "\n",
    "print(f\"\\nGLOBAL percentages:\")\n",
    "range_violations = 0\n",
    "for fold in range(1, N_FOLDS + 1):\n",
    "    fold_data = fold_info_df[fold_info_df['fold'] == fold]\n",
    "    total_test = fold_data['test_duration'].sum()\n",
    "    global_test_pct = (total_test / total_dataset_duration) * 100\n",
    "    \n",
    "    in_range = ACCEPTABLE_RANGE[0]*100 <= global_test_pct <= ACCEPTABLE_RANGE[1]*100\n",
    "    status = \"✓\" if in_range else \"✗\"\n",
    "    \n",
    "    if not in_range:\n",
    "        range_violations += 1\n",
    "    \n",
    "    print(f\"  Fold {fold}: Test={global_test_pct:.1f}% {status}\")\n",
    "\n",
    "if range_violations == 0:\n",
    "    print(f\"\\n✓ ALL FOLDS WITHIN RANGE!\")\n",
    "\n",
    "fold_info_file = os.path.join(OUTPUT_DIR, 'fold_info_per_recording.csv')\n",
    "fold_info_df.to_csv(fold_info_file, index=False)\n",
    "\n",
    "# =====================================================\n",
    "# STEP 5: CREATE WINDOWS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: CREATE WINDOWS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cv_splits = []\n",
    "\n",
    "for fold in range(1, N_FOLDS + 1):\n",
    "    print(f\"\\nFold {fold}/{N_FOLDS}...\", end=\" \")\n",
    "    \n",
    "    X_train, y_train, X_val, y_val = create_windows_for_fold(\n",
    "        fold, \n",
    "        segments_by_recording, \n",
    "        fold_info_df,\n",
    "        overlap=OVERLAP\n",
    "    )\n",
    "    \n",
    "    # Activity distribution in this fold\n",
    "    train_activity_counts = {aid: np.sum(y_train == aid) for aid in np.unique(y_train)}\n",
    "    val_activity_counts = {aid: np.sum(y_val == aid) for aid in np.unique(y_val)}\n",
    "    \n",
    "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}\")\n",
    "    \n",
    "    # Show class distribution\n",
    "    print(f\"    Train distribution:\")\n",
    "    for aid in sorted(train_activity_counts.keys()):\n",
    "        count = train_activity_counts[aid]\n",
    "        pct = count / len(y_train) * 100\n",
    "        print(f\"      Activity {aid:2d}: {count:4d} ({pct:4.1f}%)\")\n",
    "    \n",
    "    cv_splits.append({\n",
    "        'fold': fold,\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_val': X_val,\n",
    "        'y_val': y_val,\n",
    "        'n_test_windows': len(X_val),\n",
    "        'n_train_windows': len(X_train),\n",
    "        'train_activity_counts': train_activity_counts,\n",
    "        'val_activity_counts': val_activity_counts\n",
    "    })\n",
    "    \n",
    "    del X_train, X_val\n",
    "    gc.collect()\n",
    "\n",
    "cv_file = os.path.join(OUTPUT_DIR, 'cv_splits.pkl')\n",
    "with open(cv_file, 'wb') as f:\n",
    "    pickle.dump(cv_splits, f)\n",
    "print(f\"\\n✓ Saved: {cv_file}\")\n",
    "\n",
    "# =====================================================\n",
    "# SAVE METADATA\n",
    "# =====================================================\n",
    "\n",
    "metadata = {\n",
    "    'label_type': 'multi_class',\n",
    "    'n_classes': len(activities_in_data),\n",
    "    'activity_ids': activities_in_data,\n",
    "    'window_size_sec': WINDOW_SIZE,\n",
    "    'window_samples': WINDOW_SAMPLES,\n",
    "    'overlap': OVERLAP,\n",
    "    'sampling_rate': FS,\n",
    "    'n_channels': 16,\n",
    "    'n_folds': N_FOLDS,\n",
    "    'preprocessing': 'z_score_normalization_per_channel',\n",
    "    'cpd_params': {\n",
    "        'model': CPD_MODEL,\n",
    "        'min_size': CPD_MIN_SIZE,\n",
    "        'jump': CPD_JUMP,\n",
    "        'penalty': CPD_PEN\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_file = os.path.join(OUTPUT_DIR, 'dataset_metadata.pkl')\n",
    "with open(metadata_file, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOutput: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e36203d",
   "metadata": {},
   "source": [
    "## EEGNet for 10 class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b78df90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-16 13:55:29.479643: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPU configured with mixed precision\n",
      "======================================================================\n",
      "EEGNET: 10-CLASS CLASSIFICATION\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Data: /home/jupyter-yin10/EEG_HAR/Pipeline_experiments/data/windowed_zscore_temporal5fold_50overlap_10class\n",
      "  Output: /home/jupyter-yin10/EEG_HAR/Pipeline_experiments/results/eegnet_10class_zscore_50overlap\n",
      "  Classes: 10 (multi-class)\n",
      "\n",
      "Training parameters:\n",
      "  Epochs: 100, LR: 0.001, Patience: 15\n",
      "\n",
      "======================================================================\n",
      "LOADING CV SPLITS\n",
      "======================================================================\n",
      "✓ Loaded 5 folds\n",
      "  Number of classes: 10\n",
      "  Activity IDs: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10)]\n",
      "\n",
      "  Fold 1:\n",
      "    Train: 6619 windows\n",
      "    Val:   1634 windows\n",
      "\n",
      "  Fold 2:\n",
      "    Train: 6522 windows\n",
      "    Val:   1734 windows\n",
      "\n",
      "  Fold 3:\n",
      "    Train: 6484 windows\n",
      "    Val:   1751 windows\n",
      "\n",
      "  Fold 4:\n",
      "    Train: 6476 windows\n",
      "    Val:   1739 windows\n",
      "\n",
      "  Fold 5:\n",
      "    Train: 6426 windows\n",
      "    Val:   1745 windows\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT: EEGNET 10-CLASS\n",
      "======================================================================\n",
      "\n",
      "Fold 1/5...\n",
      "  Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765911331.756382  570792 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14601 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:0c:00.0, compute capability: 8.6\n",
      "I0000 00:00:1765911331.756788  570792 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 23783 MB memory:  -> device: 1, name: Quadro P6000, pci bus id: 0000:0b:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training...\n",
      "    Class weights computed for 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-16 13:55:33.650526: I external/local_xla/xla/service/service.cc:163] XLA service 0x7f6104805420 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-12-16 13:55:33.650541: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2025-12-16 13:55:33.650545: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (1): Quadro P6000, Compute Capability 6.1\n",
      "2025-12-16 13:55:33.676685: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-12-16 13:55:33.878001: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n",
      "I0000 00:00:1765911336.732865  571376 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy:     0.5208\n",
      "    Macro F1:     0.5302\n",
      "    Weighted F1:  0.5240\n",
      "  ✓ Saved model\n",
      "\n",
      "Fold 2/5...\n",
      "  Building model...\n",
      "  Training...\n",
      "    Class weights computed for 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy:     0.7751\n",
      "    Macro F1:     0.7755\n",
      "    Weighted F1:  0.7744\n",
      "  ✓ Saved model\n",
      "\n",
      "Fold 3/5...\n",
      "  Building model...\n",
      "  Training...\n",
      "    Class weights computed for 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy:     0.8058\n",
      "    Macro F1:     0.8087\n",
      "    Weighted F1:  0.8054\n",
      "  ✓ Saved model\n",
      "\n",
      "Fold 4/5...\n",
      "  Building model...\n",
      "  Training...\n",
      "    Class weights computed for 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy:     0.8194\n",
      "    Macro F1:     0.8228\n",
      "    Weighted F1:  0.8206\n",
      "  ✓ Saved model\n",
      "\n",
      "Fold 5/5...\n",
      "  Building model...\n",
      "  Training...\n",
      "    Class weights computed for 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy:     0.7874\n",
      "    Macro F1:     0.7916\n",
      "    Weighted F1:  0.7869\n",
      "  ✓ Saved model\n",
      "\n",
      "✓ Experiment Complete:\n",
      "\n",
      "  Standard Mean:\n",
      "    Accuracy:        0.7417 ± 0.1115\n",
      "    Macro F1:        0.7458 ± 0.1089\n",
      "    Weighted F1:     0.7423 ± 0.1103\n",
      "    Macro Precision: 0.7565\n",
      "    Macro Recall:    0.7475\n",
      "\n",
      "  Weighted Mean:\n",
      "    Accuracy:        0.7445 ± 0.1094\n",
      "    Macro F1:        0.7485 ± 0.1069\n",
      "    Weighted F1:     0.7450 ± 0.1082\n",
      "\n",
      "======================================================================\n",
      "AGGREGATE CONFUSION MATRIX\n",
      "======================================================================\n",
      "✓ Saved confusion matrix: /home/jupyter-yin10/EEG_HAR/Pipeline_experiments/results/eegnet_10class_zscore_50overlap/confusion_matrix.png\n",
      "\n",
      "======================================================================\n",
      "PER-CLASS F1 SCORES\n",
      "======================================================================\n",
      "\n",
      "Activity-wise F1 scores:\n",
      "  Activity  1: 0.7259 ± 0.1024\n",
      "  Activity  2: 0.7147 ± 0.1140\n",
      "  Activity  3: 0.7791 ± 0.1270\n",
      "  Activity  4: 0.8643 ± 0.0494\n",
      "  Activity  5: 0.7389 ± 0.1420\n",
      "  Activity  6: 0.7438 ± 0.1116\n",
      "  Activity  7: 0.6555 ± 0.1293\n",
      "  Activity  8: 0.8672 ± 0.1219\n",
      "  Activity  9: 0.6793 ± 0.1233\n",
      "  Activity 10: 0.6889 ± 0.1139\n",
      "\n",
      "======================================================================\n",
      "SAVING RESULTS\n",
      "======================================================================\n",
      "✓ Saved: /home/jupyter-yin10/EEG_HAR/Pipeline_experiments/results/eegnet_10class_zscore_50overlap/eegnet_10class_results_20251216_140133.pkl\n",
      "\n",
      "======================================================================\n",
      "COMPLETED SUCCESSFULLY\n",
      "======================================================================\n",
      "\n",
      "Results saved to: /home/jupyter-yin10/EEG_HAR/Pipeline_experiments/results/eegnet_10class_zscore_50overlap\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "# =====================================================\n",
    "# GPU CONFIGURATION\n",
    "# =====================================================\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "        tf.keras.mixed_precision.set_global_policy(policy)\n",
    "        print(\"✓ GPU configured with mixed precision\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU error: {e}\")\n",
    "\n",
    "# =====================================================\n",
    "# CONFIGURATION\n",
    "# =====================================================\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = '/home/jupyter-yin10/EEG_HAR/Pipeline_experiments/data/windowed_zscore_temporal5fold_50overlap_10class'\n",
    "OUTPUT_DIR = '/home/jupyter-yin10/EEG_HAR/Pipeline_experiments/results/eegnet_10class_zscore_50overlap'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Model parameters (same as before)\n",
    "CHANNELS = 16\n",
    "SAMPLES = 500\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "PATIENCE = 15\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EEGNET: 10-CLASS CLASSIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Data: {DATA_DIR}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")\n",
    "print(f\"  Classes: 10 (multi-class)\")\n",
    "\n",
    "print(f\"\\nTraining parameters:\")\n",
    "print(f\"  Epochs: {EPOCHS}, LR: {LEARNING_RATE}, Patience: {PATIENCE}\")\n",
    "\n",
    "# =====================================================\n",
    "# LOAD CV SPLITS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING CV SPLITS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'cv_splits.pkl'), 'rb') as f:\n",
    "    cv_splits = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'dataset_metadata.pkl'), 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "N_CLASSES = metadata['n_classes']\n",
    "ACTIVITY_IDS = metadata['activity_ids']\n",
    "\n",
    "print(f\"✓ Loaded {len(cv_splits)} folds\")\n",
    "print(f\"  Number of classes: {N_CLASSES}\")\n",
    "print(f\"  Activity IDs: {ACTIVITY_IDS}\")\n",
    "\n",
    "for fold_data in cv_splits:\n",
    "    fold_num = fold_data['fold']\n",
    "    print(f\"\\n  Fold {fold_num}:\")\n",
    "    print(f\"    Train: {len(fold_data['y_train'])} windows\")\n",
    "    print(f\"    Val:   {len(fold_data['y_val'])} windows\")\n",
    "\n",
    "# =====================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =====================================================\n",
    "\n",
    "def compute_metrics_multiclass(y_true, y_pred, y_pred_proba):\n",
    "    \"\"\"Compute comprehensive metrics for multi-class classification.\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'macro_precision': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'macro_recall': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'macro_f1': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'weighted_precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'weighted_recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'weighted_f1': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "    }\n",
    "    \n",
    "    # Per-class metrics\n",
    "    per_class_f1 = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    metrics['per_class_f1'] = {int(aid): float(f1) for aid, f1 in zip(ACTIVITY_IDS, per_class_f1)}\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics['confusion_matrix'] = cm\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_weighted_metrics(fold_results):\n",
    "    \"\"\"Compute weighted metrics across folds.\"\"\"\n",
    "    total_test = sum(r['n_test_windows'] for r in fold_results)\n",
    "    weights = np.array([r['n_test_windows'] / total_test for r in fold_results])\n",
    "    \n",
    "    metrics = {}\n",
    "    for metric_name in ['accuracy', 'macro_f1', 'weighted_f1', 'macro_precision', \n",
    "                       'macro_recall', 'weighted_precision', 'weighted_recall']:\n",
    "        values = np.array([r['metrics'][metric_name] for r in fold_results])\n",
    "        \n",
    "        weighted_mean = np.sum(values * weights)\n",
    "        weighted_var = np.sum(weights * (values - weighted_mean)**2)\n",
    "        weighted_std = np.sqrt(weighted_var)\n",
    "        \n",
    "        metrics[metric_name] = {\n",
    "            'mean': weighted_mean,\n",
    "            'std': weighted_std\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# =====================================================\n",
    "# MODEL BUILDING\n",
    "# =====================================================\n",
    "\n",
    "def build_eegnet(n_classes=10, channels=16, samples=500, dropout=0.5):\n",
    "    \"\"\"Build EEGNet for multi-class classification.\"\"\"\n",
    "    F1, D, F2 = 8, 2, 16\n",
    "    kernel_length = 64\n",
    "    \n",
    "    input_shape = (samples, channels, 1)\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Block 1\n",
    "    block1 = layers.Conv2D(F1, (kernel_length, 1), padding='same', use_bias=False)(input_layer)\n",
    "    block1 = layers.BatchNormalization()(block1)\n",
    "    block1 = layers.DepthwiseConv2D((1, channels), use_bias=False, depth_multiplier=D,\n",
    "                                   depthwise_constraint=keras.constraints.max_norm(1.))(block1)\n",
    "    block1 = layers.BatchNormalization()(block1)\n",
    "    block1 = layers.Activation('elu')(block1)\n",
    "    block1 = layers.AveragePooling2D((4, 1))(block1)\n",
    "    block1 = layers.Dropout(dropout)(block1)\n",
    "    \n",
    "    # Block 2\n",
    "    block2 = layers.SeparableConv2D(F2, (16, 1), use_bias=False, padding='same')(block1)\n",
    "    block2 = layers.BatchNormalization()(block2)\n",
    "    block2 = layers.Activation('elu')(block2)\n",
    "    block2 = layers.AveragePooling2D((8, 1))(block2)\n",
    "    block2 = layers.Dropout(dropout)(block2)\n",
    "    \n",
    "    # Classification\n",
    "    flatten = layers.Flatten()(block2)\n",
    "    dense = layers.Dense(n_classes, kernel_constraint=keras.constraints.max_norm(0.25))(flatten)\n",
    "    softmax = layers.Activation('softmax')(dense)\n",
    "    \n",
    "    model = models.Model(inputs=input_layer, outputs=softmax)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, lr, epochs, fold_name, use_class_weights=True):\n",
    "    \"\"\"Train model and return results.\"\"\"\n",
    "    \n",
    "    X_train_reshaped = X_train.reshape(len(X_train), SAMPLES, CHANNELS, 1)\n",
    "    X_val_reshaped = X_val.reshape(len(X_val), SAMPLES, CHANNELS, 1)\n",
    "    \n",
    "    y_train_cat = to_categorical(y_train - 1, N_CLASSES)  # Convert 1-10 to 0-9\n",
    "    y_val_cat = to_categorical(y_val - 1, N_CLASSES)\n",
    "    \n",
    "    class_weight_dict = None\n",
    "    if use_class_weights:\n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "        print(f\"    Class weights computed for {len(class_weight_dict)} classes\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_reshaped, y_train_cat,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_val_reshaped, y_val_cat),\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    y_pred_proba = model.predict(X_val_reshaped, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1) + 1  # Convert back to 1-10\n",
    "    \n",
    "    metrics = compute_metrics_multiclass(y_val, y_pred, y_pred_proba)\n",
    "    \n",
    "    print(f\"    Accuracy:     {metrics['accuracy']:.4f}\")\n",
    "    print(f\"    Macro F1:     {metrics['macro_f1']:.4f}\")\n",
    "    print(f\"    Weighted F1:  {metrics['weighted_f1']:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'history': history.history,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'metrics': metrics,\n",
    "        'epochs_trained': len(history.history['loss']),\n",
    "        'n_test_windows': len(y_val),\n",
    "        'class_weights': class_weight_dict\n",
    "    }\n",
    "\n",
    "# =====================================================\n",
    "# EXPERIMENT: EEGNET 10-CLASS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT: EEGNET 10-CLASS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "exp_results = []\n",
    "\n",
    "for fold_data in cv_splits:\n",
    "    fold_num = fold_data['fold']\n",
    "    \n",
    "    print(f\"\\nFold {fold_num}/{len(cv_splits)}...\")\n",
    "    \n",
    "    X_train = fold_data['X_train']\n",
    "    y_train = fold_data['y_train']\n",
    "    X_val = fold_data['X_val']\n",
    "    y_val = fold_data['y_val']\n",
    "    \n",
    "    print(f\"  Building model...\")\n",
    "    model = build_eegnet(n_classes=N_CLASSES, channels=CHANNELS, \n",
    "                        samples=SAMPLES, dropout=DROPOUT)\n",
    "    \n",
    "    print(f\"  Training...\")\n",
    "    results = train_model(model, X_train, y_train, X_val, y_val,\n",
    "                         lr=LEARNING_RATE, epochs=EPOCHS, \n",
    "                         fold_name=f'eegnet_10class_fold{fold_num}',\n",
    "                         use_class_weights=True)\n",
    "    \n",
    "    results['fold'] = fold_num\n",
    "    exp_results.append(results)\n",
    "    \n",
    "    fold_dir = os.path.join(OUTPUT_DIR, f'fold_{fold_num}')\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "    model.save(os.path.join(fold_dir, 'eegnet_10class.h5'))\n",
    "    print(f\"  ✓ Saved model\")\n",
    "    \n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "# Compute averages\n",
    "exp_avg_standard = {\n",
    "    'accuracy': np.mean([r['metrics']['accuracy'] for r in exp_results]),\n",
    "    'macro_f1': np.mean([r['metrics']['macro_f1'] for r in exp_results]),\n",
    "    'weighted_f1': np.mean([r['metrics']['weighted_f1'] for r in exp_results]),\n",
    "    'macro_precision': np.mean([r['metrics']['macro_precision'] for r in exp_results]),\n",
    "    'macro_recall': np.mean([r['metrics']['macro_recall'] for r in exp_results]),\n",
    "    'weighted_precision': np.mean([r['metrics']['weighted_precision'] for r in exp_results]),\n",
    "    'weighted_recall': np.mean([r['metrics']['weighted_recall'] for r in exp_results]),\n",
    "    'std_accuracy': np.std([r['metrics']['accuracy'] for r in exp_results]),\n",
    "    'std_macro_f1': np.std([r['metrics']['macro_f1'] for r in exp_results]),\n",
    "    'std_weighted_f1': np.std([r['metrics']['weighted_f1'] for r in exp_results]),\n",
    "}\n",
    "\n",
    "exp_avg_weighted = compute_weighted_metrics(exp_results)\n",
    "\n",
    "print(f\"\\n✓ Experiment Complete:\")\n",
    "print(f\"\\n  Standard Mean:\")\n",
    "print(f\"    Accuracy:        {exp_avg_standard['accuracy']:.4f} ± {exp_avg_standard['std_accuracy']:.4f}\")\n",
    "print(f\"    Macro F1:        {exp_avg_standard['macro_f1']:.4f} ± {exp_avg_standard['std_macro_f1']:.4f}\")\n",
    "print(f\"    Weighted F1:     {exp_avg_standard['weighted_f1']:.4f} ± {exp_avg_standard['std_weighted_f1']:.4f}\")\n",
    "print(f\"    Macro Precision: {exp_avg_standard['macro_precision']:.4f}\")\n",
    "print(f\"    Macro Recall:    {exp_avg_standard['macro_recall']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Weighted Mean:\")\n",
    "print(f\"    Accuracy:        {exp_avg_weighted['accuracy']['mean']:.4f} ± {exp_avg_weighted['accuracy']['std']:.4f}\")\n",
    "print(f\"    Macro F1:        {exp_avg_weighted['macro_f1']['mean']:.4f} ± {exp_avg_weighted['macro_f1']['std']:.4f}\")\n",
    "print(f\"    Weighted F1:     {exp_avg_weighted['weighted_f1']['mean']:.4f} ± {exp_avg_weighted['weighted_f1']['std']:.4f}\")\n",
    "\n",
    "# =====================================================\n",
    "# AGGREGATE CONFUSION MATRIX\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"AGGREGATE CONFUSION MATRIX\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sum confusion matrices across folds\n",
    "cm_sum = np.zeros((N_CLASSES, N_CLASSES))\n",
    "for result in exp_results:\n",
    "    cm_sum += result['metrics']['confusion_matrix']\n",
    "\n",
    "# Normalize\n",
    "cm_normalized = cm_sum / cm_sum.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=ACTIVITY_IDS, yticklabels=ACTIVITY_IDS)\n",
    "plt.xlabel('Predicted Activity', fontsize=12)\n",
    "plt.ylabel('True Activity', fontsize=12)\n",
    "plt.title('Normalized Confusion Matrix (Aggregated Across Folds)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "cm_file = os.path.join(OUTPUT_DIR, 'confusion_matrix.png')\n",
    "plt.savefig(cm_file, dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Saved confusion matrix: {cm_file}\")\n",
    "plt.close()\n",
    "\n",
    "# =====================================================\n",
    "# PER-CLASS F1 SCORES\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"PER-CLASS F1 SCORES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Average per-class F1 across folds\n",
    "per_class_f1_avg = {}\n",
    "for aid in ACTIVITY_IDS:\n",
    "    f1_scores = [r['metrics']['per_class_f1'][aid] for r in exp_results]\n",
    "    per_class_f1_avg[aid] = {\n",
    "        'mean': np.mean(f1_scores),\n",
    "        'std': np.std(f1_scores)\n",
    "    }\n",
    "\n",
    "print(f\"\\nActivity-wise F1 scores:\")\n",
    "for aid in ACTIVITY_IDS:\n",
    "    mean_f1 = per_class_f1_avg[aid]['mean']\n",
    "    std_f1 = per_class_f1_avg[aid]['std']\n",
    "    print(f\"  Activity {aid:2d}: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "\n",
    "# =====================================================\n",
    "# SAVE RESULTS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "all_results = {\n",
    "    'model': 'EEGNet',\n",
    "    'n_classes': N_CLASSES,\n",
    "    'activity_ids': ACTIVITY_IDS,\n",
    "    'overlap_version': '50%',\n",
    "    'preprocessing': 'z_score_normalization',\n",
    "    'cv_strategy': 'temporal_5fold',\n",
    "    'exp_results': exp_results,\n",
    "    'exp_avg_standard': exp_avg_standard,\n",
    "    'exp_avg_weighted': exp_avg_weighted,\n",
    "    'per_class_f1_avg': per_class_f1_avg,\n",
    "    'confusion_matrix_aggregated': cm_sum.tolist(),\n",
    "    'confusion_matrix_normalized': cm_normalized.tolist(),\n",
    "    'timestamp': timestamp,\n",
    "    'config': {\n",
    "        'data_dir': DATA_DIR,\n",
    "        'n_folds': len(cv_splits),\n",
    "        'epochs': EPOCHS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'patience': PATIENCE,\n",
    "        'dropout': DROPOUT\n",
    "    }\n",
    "}\n",
    "\n",
    "results_file = os.path.join(OUTPUT_DIR, f'eegnet_10class_results_{timestamp}.pkl')\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "\n",
    "print(f\"✓ Saved: {results_file}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nResults saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928085bc",
   "metadata": {},
   "source": [
    "## EEGNet + LSTM for 10 class classificaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ede853c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPU configured (float32 for LSTM speed)\n",
      "======================================================================\n",
      "EEGNET-LSTM: 10-CLASS CLASSIFICATION\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Data: /home/jupyter-yin10/EEG_HAR/Pipeline_experiments/data/windowed_zscore_temporal5fold_50overlap_10class\n",
      "  Output: /home/jupyter-yin10/EEG_HAR/Pipeline_experiments/results/eegnet_lstm_10class_zscore_50overlap\n",
      "  Classes: 10 (multi-class)\n",
      "\n",
      "Model parameters:\n",
      "  EEGNet: F1=8, D=2, F2=16, dropout=0.5\n",
      "  LSTM: [128, 64] units, dropout=0.3\n",
      "\n",
      "Training parameters:\n",
      "  Epochs: 100, LR: 0.001, Patience: 15\n",
      "\n",
      "======================================================================\n",
      "LOADING CV SPLITS\n",
      "======================================================================\n",
      "✓ Loaded 5 folds\n",
      "  Number of classes: 10\n",
      "  Activity IDs: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10)]\n",
      "\n",
      "  Fold 1:\n",
      "    Train: 6619 windows\n",
      "    Val:   1634 windows\n",
      "\n",
      "  Fold 2:\n",
      "    Train: 6522 windows\n",
      "    Val:   1734 windows\n",
      "\n",
      "  Fold 3:\n",
      "    Train: 6484 windows\n",
      "    Val:   1751 windows\n",
      "\n",
      "  Fold 4:\n",
      "    Train: 6476 windows\n",
      "    Val:   1739 windows\n",
      "\n",
      "  Fold 5:\n",
      "    Train: 6426 windows\n",
      "    Val:   1745 windows\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT: EEGNET-LSTM 10-CLASS\n",
      "======================================================================\n",
      "\n",
      "Fold 1/5...\n",
      "  Building EEGNet-LSTM model...\n",
      "    Total params: 125,738\n",
      "  Training...\n",
      "    Class weights computed for 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1765911718.850784  570792 meta_optimizer.cc:967] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/functional_1/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy:     0.7436\n",
      "    Macro F1:     0.7375\n",
      "    Weighted F1:  0.7424\n",
      "  ✓ Saved model\n",
      "\n",
      "Fold 2/5...\n",
      "  Building EEGNet-LSTM model...\n",
      "    Total params: 125,738\n",
      "  Training...\n",
      "    Class weights computed for 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1765911926.838309  570792 meta_optimizer.cc:967] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/functional_1/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy:     0.9556\n",
      "    Macro F1:     0.9553\n",
      "    Weighted F1:  0.9557\n",
      "  ✓ Saved model\n",
      "\n",
      "Fold 3/5...\n",
      "  Building EEGNet-LSTM model...\n",
      "    Total params: 125,738\n",
      "  Training...\n",
      "    Class weights computed for 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1765912127.226066  570792 meta_optimizer.cc:967] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/functional_1/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy:     0.9423\n",
      "    Macro F1:     0.9405\n",
      "    Weighted F1:  0.9422\n",
      "  ✓ Saved model\n",
      "\n",
      "Fold 4/5...\n",
      "  Building EEGNet-LSTM model...\n",
      "    Total params: 125,738\n",
      "  Training...\n",
      "    Class weights computed for 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1765912330.381202  570792 meta_optimizer.cc:967] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/functional_1/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy:     0.9770\n",
      "    Macro F1:     0.9773\n",
      "    Weighted F1:  0.9770\n",
      "  ✓ Saved model\n",
      "\n",
      "Fold 5/5...\n",
      "  Building EEGNet-LSTM model...\n",
      "    Total params: 125,738\n",
      "  Training...\n",
      "    Class weights computed for 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1765912743.804209  570792 meta_optimizer.cc:967] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/functional_1/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy:     0.9817\n",
      "    Macro F1:     0.9812\n",
      "    Weighted F1:  0.9817\n",
      "  ✓ Saved model\n",
      "\n",
      "✓ Experiment Complete:\n",
      "\n",
      "  Standard Mean:\n",
      "    Accuracy:        0.9200 ± 0.0894\n",
      "    Macro F1:        0.9184 ± 0.0916\n",
      "    Weighted F1:     0.9198 ± 0.0899\n",
      "    Macro Precision: 0.9199\n",
      "    Macro Recall:    0.9181\n",
      "\n",
      "  Weighted Mean:\n",
      "    Accuracy:        0.9222 ± 0.0877\n",
      "    Macro F1:        0.9206 ± 0.0899\n",
      "    Weighted F1:     0.9220 ± 0.0882\n",
      "\n",
      "======================================================================\n",
      "AGGREGATE CONFUSION MATRIX\n",
      "======================================================================\n",
      "✓ Saved confusion matrix: /home/jupyter-yin10/EEG_HAR/Pipeline_experiments/results/eegnet_lstm_10class_zscore_50overlap/confusion_matrix.png\n",
      "\n",
      "======================================================================\n",
      "PER-CLASS F1 SCORES\n",
      "======================================================================\n",
      "\n",
      "Activity-wise F1 scores:\n",
      "  Activity  1: 0.9189 ± 0.0848\n",
      "  Activity  2: 0.8904 ± 0.1124\n",
      "  Activity  3: 0.9183 ± 0.0905\n",
      "  Activity  4: 0.8999 ± 0.1265\n",
      "  Activity  5: 0.9520 ± 0.0556\n",
      "  Activity  6: 0.9220 ± 0.0572\n",
      "  Activity  7: 0.9422 ± 0.0788\n",
      "  Activity  8: 0.9019 ± 0.1280\n",
      "  Activity  9: 0.9111 ± 0.1042\n",
      "  Activity 10: 0.9269 ± 0.0958\n",
      "\n",
      "======================================================================\n",
      "SAVING RESULTS\n",
      "======================================================================\n",
      "✓ Saved: /home/jupyter-yin10/EEG_HAR/Pipeline_experiments/results/eegnet_lstm_10class_zscore_50overlap/eegnet_lstm_10class_results_20251216_142835.pkl\n",
      "\n",
      "======================================================================\n",
      "COMPLETED SUCCESSFULLY\n",
      "======================================================================\n",
      "\n",
      "Results saved to: /home/jupyter-yin10/EEG_HAR/Pipeline_experiments/results/eegnet_lstm_10class_zscore_50overlap\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "# =====================================================\n",
    "# GPU CONFIGURATION\n",
    "# =====================================================\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        # Disable mixed precision for LSTM (avoids slowdown)\n",
    "        tf.keras.mixed_precision.set_global_policy('float32')\n",
    "        print(\"✓ GPU configured (float32 for LSTM speed)\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU error: {e}\")\n",
    "\n",
    "# =====================================================\n",
    "# CONFIGURATION\n",
    "# =====================================================\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = '/home/jupyter-yin10/EEG_HAR/Pipeline_experiments/data/windowed_zscore_temporal5fold_50overlap_10class'\n",
    "OUTPUT_DIR = '/home/jupyter-yin10/EEG_HAR/Pipeline_experiments/results/eegnet_lstm_10class_zscore_50overlap'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Model parameters (same as before)\n",
    "CHANNELS = 16\n",
    "SAMPLES = 500\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# LSTM parameters (same as before)\n",
    "LSTM_UNITS_1 = 128\n",
    "LSTM_UNITS_2 = 64\n",
    "LSTM_DROPOUT = 0.3\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "PATIENCE = 15\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EEGNET-LSTM: 10-CLASS CLASSIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Data: {DATA_DIR}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")\n",
    "print(f\"  Classes: 10 (multi-class)\")\n",
    "\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"  EEGNet: F1=8, D=2, F2=16, dropout={DROPOUT}\")\n",
    "print(f\"  LSTM: [{LSTM_UNITS_1}, {LSTM_UNITS_2}] units, dropout={LSTM_DROPOUT}\")\n",
    "\n",
    "print(f\"\\nTraining parameters:\")\n",
    "print(f\"  Epochs: {EPOCHS}, LR: {LEARNING_RATE}, Patience: {PATIENCE}\")\n",
    "\n",
    "# =====================================================\n",
    "# LOAD CV SPLITS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING CV SPLITS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'cv_splits.pkl'), 'rb') as f:\n",
    "    cv_splits = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'dataset_metadata.pkl'), 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "N_CLASSES = metadata['n_classes']\n",
    "ACTIVITY_IDS = metadata['activity_ids']\n",
    "\n",
    "print(f\"✓ Loaded {len(cv_splits)} folds\")\n",
    "print(f\"  Number of classes: {N_CLASSES}\")\n",
    "print(f\"  Activity IDs: {ACTIVITY_IDS}\")\n",
    "\n",
    "for fold_data in cv_splits:\n",
    "    fold_num = fold_data['fold']\n",
    "    print(f\"\\n  Fold {fold_num}:\")\n",
    "    print(f\"    Train: {len(fold_data['y_train'])} windows\")\n",
    "    print(f\"    Val:   {len(fold_data['y_val'])} windows\")\n",
    "\n",
    "# =====================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =====================================================\n",
    "\n",
    "def compute_metrics_multiclass(y_true, y_pred, y_pred_proba):\n",
    "    \"\"\"Compute comprehensive metrics for multi-class classification.\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'macro_precision': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'macro_recall': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'macro_f1': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'weighted_precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'weighted_recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'weighted_f1': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "    }\n",
    "    \n",
    "    # Per-class metrics\n",
    "    per_class_f1 = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    metrics['per_class_f1'] = {int(aid): float(f1) for aid, f1 in zip(ACTIVITY_IDS, per_class_f1)}\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics['confusion_matrix'] = cm\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_weighted_metrics(fold_results):\n",
    "    \"\"\"Compute weighted metrics across folds.\"\"\"\n",
    "    total_test = sum(r['n_test_windows'] for r in fold_results)\n",
    "    weights = np.array([r['n_test_windows'] / total_test for r in fold_results])\n",
    "    \n",
    "    metrics = {}\n",
    "    for metric_name in ['accuracy', 'macro_f1', 'weighted_f1', 'macro_precision', \n",
    "                       'macro_recall', 'weighted_precision', 'weighted_recall']:\n",
    "        values = np.array([r['metrics'][metric_name] for r in fold_results])\n",
    "        \n",
    "        weighted_mean = np.sum(values * weights)\n",
    "        weighted_var = np.sum(weights * (values - weighted_mean)**2)\n",
    "        weighted_std = np.sqrt(weighted_var)\n",
    "        \n",
    "        metrics[metric_name] = {\n",
    "            'mean': weighted_mean,\n",
    "            'std': weighted_std\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# =====================================================\n",
    "# MODEL BUILDING\n",
    "# =====================================================\n",
    "\n",
    "def build_eegnet_lstm(n_classes=10, channels=16, samples=500, \n",
    "                      eegnet_dropout=0.5, lstm_dropout=0.3,\n",
    "                      lstm_units_1=128, lstm_units_2=64):\n",
    "    \"\"\"\n",
    "    Build EEGNet-LSTM architecture for multi-class classification.\n",
    "    \n",
    "    CRITICAL: Use recurrent_dropout=0.0 for CuDNN fast path!\n",
    "    Use SpatialDropout1D instead for regularization.\n",
    "    \"\"\"\n",
    "    F1, D, F2 = 8, 2, 16\n",
    "    kernel_length = 64\n",
    "    \n",
    "    input_shape = (samples, channels, 1)\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # ==================== EEGNet Block 1 ====================\n",
    "    block1 = layers.Conv2D(F1, (kernel_length, 1), padding='same', use_bias=False)(input_layer)\n",
    "    block1 = layers.BatchNormalization()(block1)\n",
    "    block1 = layers.DepthwiseConv2D((1, channels), use_bias=False, depth_multiplier=D,\n",
    "                                   depthwise_constraint=keras.constraints.max_norm(1.))(block1)\n",
    "    block1 = layers.BatchNormalization()(block1)\n",
    "    block1 = layers.Activation('elu')(block1)\n",
    "    block1 = layers.AveragePooling2D((4, 1))(block1)\n",
    "    block1 = layers.Dropout(eegnet_dropout)(block1)\n",
    "    \n",
    "    # ==================== EEGNet Block 2 ====================\n",
    "    block2 = layers.SeparableConv2D(F2, (16, 1), use_bias=False, padding='same')(block1)\n",
    "    block2 = layers.BatchNormalization()(block2)\n",
    "    block2 = layers.Activation('elu')(block2)\n",
    "    block2 = layers.AveragePooling2D((8, 1))(block2)\n",
    "    block2 = layers.Dropout(eegnet_dropout)(block2)\n",
    "    \n",
    "    # ==================== Reshape for LSTM ====================\n",
    "    # Output shape: (batch, time_steps, features)\n",
    "    # block2 shape: (batch, 15, 1, 16) → reshape to (batch, 15, 16)\n",
    "    reshape = layers.Reshape((15, 16))(block2)\n",
    "    \n",
    "    # ==================== LSTM Layers ====================\n",
    "    # CRITICAL: recurrent_dropout=0.0 for CuDNN speed!\n",
    "    lstm1 = layers.LSTM(\n",
    "        lstm_units_1,\n",
    "        return_sequences=True,\n",
    "        dropout=0.0,  # Use SpatialDropout1D instead\n",
    "        recurrent_dropout=0.0,  # MUST be 0 for CuDNN!\n",
    "        name='lstm_1'\n",
    "    )(reshape)\n",
    "    lstm1 = layers.SpatialDropout1D(lstm_dropout)(lstm1)  # Dropout after LSTM\n",
    "    \n",
    "    lstm2 = layers.LSTM(\n",
    "        lstm_units_2,\n",
    "        return_sequences=False,\n",
    "        dropout=0.0,  # Use dropout after instead\n",
    "        recurrent_dropout=0.0,  # MUST be 0 for CuDNN!\n",
    "        name='lstm_2'\n",
    "    )(lstm1)\n",
    "    lstm2 = layers.Dropout(lstm_dropout)(lstm2)  # Dropout after LSTM\n",
    "    \n",
    "    # ==================== Classification ====================\n",
    "    dense = layers.Dense(n_classes, kernel_constraint=keras.constraints.max_norm(0.25))(lstm2)\n",
    "    softmax = layers.Activation('softmax')(dense)\n",
    "    \n",
    "    model = models.Model(inputs=input_layer, outputs=softmax)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, lr, epochs, fold_name, use_class_weights=True):\n",
    "    \"\"\"Train model and return results.\"\"\"\n",
    "    \n",
    "    X_train_reshaped = X_train.reshape(len(X_train), SAMPLES, CHANNELS, 1)\n",
    "    X_val_reshaped = X_val.reshape(len(X_val), SAMPLES, CHANNELS, 1)\n",
    "    \n",
    "    y_train_cat = to_categorical(y_train - 1, N_CLASSES)  # Convert 1-10 to 0-9\n",
    "    y_val_cat = to_categorical(y_val - 1, N_CLASSES)\n",
    "    \n",
    "    class_weight_dict = None\n",
    "    if use_class_weights:\n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "        print(f\"    Class weights computed for {len(class_weight_dict)} classes\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_reshaped, y_train_cat,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_val_reshaped, y_val_cat),\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    y_pred_proba = model.predict(X_val_reshaped, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1) + 1  # Convert back to 1-10\n",
    "    \n",
    "    metrics = compute_metrics_multiclass(y_val, y_pred, y_pred_proba)\n",
    "    \n",
    "    print(f\"    Accuracy:     {metrics['accuracy']:.4f}\")\n",
    "    print(f\"    Macro F1:     {metrics['macro_f1']:.4f}\")\n",
    "    print(f\"    Weighted F1:  {metrics['weighted_f1']:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'history': history.history,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'metrics': metrics,\n",
    "        'epochs_trained': len(history.history['loss']),\n",
    "        'n_test_windows': len(y_val),\n",
    "        'class_weights': class_weight_dict\n",
    "    }\n",
    "\n",
    "# =====================================================\n",
    "# EXPERIMENT: EEGNET-LSTM 10-CLASS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT: EEGNET-LSTM 10-CLASS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "exp_results = []\n",
    "\n",
    "for fold_data in cv_splits:\n",
    "    fold_num = fold_data['fold']\n",
    "    \n",
    "    print(f\"\\nFold {fold_num}/{len(cv_splits)}...\")\n",
    "    \n",
    "    X_train = fold_data['X_train']\n",
    "    y_train = fold_data['y_train']\n",
    "    X_val = fold_data['X_val']\n",
    "    y_val = fold_data['y_val']\n",
    "    \n",
    "    print(f\"  Building EEGNet-LSTM model...\")\n",
    "    model = build_eegnet_lstm(\n",
    "        n_classes=N_CLASSES, \n",
    "        channels=CHANNELS, \n",
    "        samples=SAMPLES, \n",
    "        eegnet_dropout=DROPOUT,\n",
    "        lstm_dropout=LSTM_DROPOUT,\n",
    "        lstm_units_1=LSTM_UNITS_1,\n",
    "        lstm_units_2=LSTM_UNITS_2\n",
    "    )\n",
    "    \n",
    "    print(f\"    Total params: {model.count_params():,}\")\n",
    "    \n",
    "    print(f\"  Training...\")\n",
    "    results = train_model(model, X_train, y_train, X_val, y_val,\n",
    "                         lr=LEARNING_RATE, epochs=EPOCHS, \n",
    "                         fold_name=f'eegnet_lstm_10class_fold{fold_num}',\n",
    "                         use_class_weights=True)\n",
    "    \n",
    "    results['fold'] = fold_num\n",
    "    exp_results.append(results)\n",
    "    \n",
    "    fold_dir = os.path.join(OUTPUT_DIR, f'fold_{fold_num}')\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "    model.save(os.path.join(fold_dir, 'eegnet_lstm_10class.h5'))\n",
    "    print(f\"  ✓ Saved model\")\n",
    "    \n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "# Compute averages\n",
    "exp_avg_standard = {\n",
    "    'accuracy': np.mean([r['metrics']['accuracy'] for r in exp_results]),\n",
    "    'macro_f1': np.mean([r['metrics']['macro_f1'] for r in exp_results]),\n",
    "    'weighted_f1': np.mean([r['metrics']['weighted_f1'] for r in exp_results]),\n",
    "    'macro_precision': np.mean([r['metrics']['macro_precision'] for r in exp_results]),\n",
    "    'macro_recall': np.mean([r['metrics']['macro_recall'] for r in exp_results]),\n",
    "    'weighted_precision': np.mean([r['metrics']['weighted_precision'] for r in exp_results]),\n",
    "    'weighted_recall': np.mean([r['metrics']['weighted_recall'] for r in exp_results]),\n",
    "    'std_accuracy': np.std([r['metrics']['accuracy'] for r in exp_results]),\n",
    "    'std_macro_f1': np.std([r['metrics']['macro_f1'] for r in exp_results]),\n",
    "    'std_weighted_f1': np.std([r['metrics']['weighted_f1'] for r in exp_results]),\n",
    "}\n",
    "\n",
    "exp_avg_weighted = compute_weighted_metrics(exp_results)\n",
    "\n",
    "print(f\"\\n✓ Experiment Complete:\")\n",
    "print(f\"\\n  Standard Mean:\")\n",
    "print(f\"    Accuracy:        {exp_avg_standard['accuracy']:.4f} ± {exp_avg_standard['std_accuracy']:.4f}\")\n",
    "print(f\"    Macro F1:        {exp_avg_standard['macro_f1']:.4f} ± {exp_avg_standard['std_macro_f1']:.4f}\")\n",
    "print(f\"    Weighted F1:     {exp_avg_standard['weighted_f1']:.4f} ± {exp_avg_standard['std_weighted_f1']:.4f}\")\n",
    "print(f\"    Macro Precision: {exp_avg_standard['macro_precision']:.4f}\")\n",
    "print(f\"    Macro Recall:    {exp_avg_standard['macro_recall']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Weighted Mean:\")\n",
    "print(f\"    Accuracy:        {exp_avg_weighted['accuracy']['mean']:.4f} ± {exp_avg_weighted['accuracy']['std']:.4f}\")\n",
    "print(f\"    Macro F1:        {exp_avg_weighted['macro_f1']['mean']:.4f} ± {exp_avg_weighted['macro_f1']['std']:.4f}\")\n",
    "print(f\"    Weighted F1:     {exp_avg_weighted['weighted_f1']['mean']:.4f} ± {exp_avg_weighted['weighted_f1']['std']:.4f}\")\n",
    "\n",
    "# =====================================================\n",
    "# AGGREGATE CONFUSION MATRIX\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"AGGREGATE CONFUSION MATRIX\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sum confusion matrices across folds\n",
    "cm_sum = np.zeros((N_CLASSES, N_CLASSES))\n",
    "for result in exp_results:\n",
    "    cm_sum += result['metrics']['confusion_matrix']\n",
    "\n",
    "# Normalize\n",
    "cm_normalized = cm_sum / cm_sum.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=ACTIVITY_IDS, yticklabels=ACTIVITY_IDS)\n",
    "plt.xlabel('Predicted Activity', fontsize=12)\n",
    "plt.ylabel('True Activity', fontsize=12)\n",
    "plt.title('Normalized Confusion Matrix (Aggregated Across Folds)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "cm_file = os.path.join(OUTPUT_DIR, 'confusion_matrix.png')\n",
    "plt.savefig(cm_file, dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Saved confusion matrix: {cm_file}\")\n",
    "plt.close()\n",
    "\n",
    "# =====================================================\n",
    "# PER-CLASS F1 SCORES\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"PER-CLASS F1 SCORES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Average per-class F1 across folds\n",
    "per_class_f1_avg = {}\n",
    "for aid in ACTIVITY_IDS:\n",
    "    f1_scores = [r['metrics']['per_class_f1'][aid] for r in exp_results]\n",
    "    per_class_f1_avg[aid] = {\n",
    "        'mean': np.mean(f1_scores),\n",
    "        'std': np.std(f1_scores)\n",
    "    }\n",
    "\n",
    "print(f\"\\nActivity-wise F1 scores:\")\n",
    "for aid in ACTIVITY_IDS:\n",
    "    mean_f1 = per_class_f1_avg[aid]['mean']\n",
    "    std_f1 = per_class_f1_avg[aid]['std']\n",
    "    print(f\"  Activity {aid:2d}: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "\n",
    "# =====================================================\n",
    "# SAVE RESULTS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "all_results = {\n",
    "    'model': 'EEGNet-LSTM',\n",
    "    'n_classes': N_CLASSES,\n",
    "    'activity_ids': ACTIVITY_IDS,\n",
    "    'overlap_version': '50%',\n",
    "    'preprocessing': 'z_score_normalization',\n",
    "    'cv_strategy': 'temporal_5fold',\n",
    "    'exp_results': exp_results,\n",
    "    'exp_avg_standard': exp_avg_standard,\n",
    "    'exp_avg_weighted': exp_avg_weighted,\n",
    "    'per_class_f1_avg': per_class_f1_avg,\n",
    "    'confusion_matrix_aggregated': cm_sum.tolist(),\n",
    "    'confusion_matrix_normalized': cm_normalized.tolist(),\n",
    "    'timestamp': timestamp,\n",
    "    'config': {\n",
    "        'data_dir': DATA_DIR,\n",
    "        'n_folds': len(cv_splits),\n",
    "        'epochs': EPOCHS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'patience': PATIENCE,\n",
    "        'eegnet_dropout': DROPOUT,\n",
    "        'lstm_dropout': LSTM_DROPOUT,\n",
    "        'lstm_units': [LSTM_UNITS_1, LSTM_UNITS_2]\n",
    "    }\n",
    "}\n",
    "\n",
    "results_file = os.path.join(OUTPUT_DIR, f'eegnet_lstm_10class_results_{timestamp}.pkl')\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "\n",
    "print(f\"✓ Saved: {results_file}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nResults saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc70b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
